{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== Optional SVD (очень желательно) =====\n",
    "try:\n",
    "    from scipy.sparse import coo_matrix\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    HAS_SVD = True\n",
    "except Exception:\n",
    "    HAS_SVD = False\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "TOPK_CAND = 20          # сколько первых книг из candidates.book_id_list брать\n",
    "HOLDOUT_K = 5           # последние K событий на пользователя = \"future\" для псевдо-теста\n",
    "MIN_HISTORY = 10        # минимум истории до holdout\n",
    "NEG_PER_USER = 15       # сколько cold негативов на пользователя (как в задаче)\n",
    "NEG_POOL = 600          # пул кандидатов для hard-negative выбора\n",
    "NEG_OVERSAMPLE = 30     # запас для фильтрации использованных книг\n",
    "\n",
    "# LGBM\n",
    "N_EST = 6000\n",
    "LR = 0.03\n",
    "NUM_LEAVES = 255\n",
    "\n",
    "# Rating-aware SVD\n",
    "USE_SVD = True\n",
    "SVD_DIM = 128\n",
    "SVD_PLAN_W = 0.25       # вес has_read=0\n",
    "# has_read=1 вес = 1 + rating/10\n",
    "\n",
    "BAYES_M = 20            # байесовский приор для среднего рейтинга книги\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD (как у тебя)\n",
    "# =========================\n",
    "def load_public(base=\"public/\"):\n",
    "    files = sorted([os.path.join(base, f) for f in os.listdir(base) if f.endswith(\".csv\")])\n",
    "    # твой порядок файлов\n",
    "    candidates = pd.read_csv(files[3])\n",
    "    train = pd.read_csv(files[7])\n",
    "    return train, candidates\n",
    "\n",
    "\n",
    "def explode_candidates(candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "    cand = candidates[[\"user_id\", \"book_id_list\"]].copy()\n",
    "    cand[\"user_id\"] = cand[\"user_id\"].astype(int)\n",
    "    cand[\"book_id_list\"] = cand[\"book_id_list\"].astype(str).apply(lambda x: \",\".join(x.split(\",\")[:TOPK_CAND]))\n",
    "\n",
    "    cand = cand.assign(book_id=cand[\"book_id_list\"].str.split(\",\")).explode(\"book_id\")\n",
    "    cand[\"book_id\"] = cand[\"book_id\"].astype(str).str.strip()\n",
    "    cand = cand[cand[\"book_id\"] != \"\"].copy()\n",
    "    cand[\"book_id\"] = cand[\"book_id\"].astype(int)\n",
    "\n",
    "    cand[\"orig_pos\"] = cand.groupby(\"user_id\").cumcount().astype(int)\n",
    "    cand[\"orig_pos_inv\"] = 1.0 / (1.0 + cand[\"orig_pos\"])\n",
    "    cand[\"orig_pos_log\"] = np.log1p(cand[\"orig_pos\"])\n",
    "\n",
    "    return cand[[\"user_id\", \"book_id\", \"orig_pos\", \"orig_pos_inv\", \"orig_pos_log\"]]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PSEUDO-FUTURE SPLIT\n",
    "# =========================\n",
    "def split_history_future(train: pd.DataFrame):\n",
    "    tr = train.copy()\n",
    "    tr[\"timestamp\"] = pd.to_datetime(tr[\"timestamp\"], errors=\"coerce\")\n",
    "    tr = tr.dropna(subset=[\"timestamp\"]).copy()\n",
    "\n",
    "    tr[\"user_id\"] = tr[\"user_id\"].astype(int)\n",
    "    tr[\"book_id\"] = tr[\"book_id\"].astype(int)\n",
    "    tr[\"has_read\"] = tr[\"has_read\"].astype(int)\n",
    "    tr[\"rating\"] = tr[\"rating\"].astype(int)\n",
    "\n",
    "    tr = tr.sort_values([\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    sizes = tr.groupby(\"user_id\").size()\n",
    "    eligible = sizes[sizes >= (MIN_HISTORY + HOLDOUT_K)].index\n",
    "    tr = tr[tr[\"user_id\"].isin(eligible)].copy()\n",
    "\n",
    "    tr[\"pos_in_user\"] = tr.groupby(\"user_id\").cumcount()\n",
    "    tr[\"n_in_user\"] = tr.groupby(\"user_id\")[\"book_id\"].transform(\"size\")\n",
    "    tr[\"is_future\"] = tr[\"pos_in_user\"] >= (tr[\"n_in_user\"] - HOLDOUT_K)\n",
    "\n",
    "    history = tr[~tr[\"is_future\"]].copy()\n",
    "    future  = tr[ tr[\"is_future\"]].copy()\n",
    "    return history, future\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FEATURE TABLES (rating-aware)\n",
    "# =========================\n",
    "def build_feature_tables(events: pd.DataFrame):\n",
    "    e = events.copy()\n",
    "    e[\"timestamp\"] = pd.to_datetime(e[\"timestamp\"], errors=\"coerce\")\n",
    "    e = e.dropna(subset=[\"timestamp\"]).copy()\n",
    "    e = e.sort_values([\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    T = e[\"timestamp\"].max()\n",
    "    read = e[e[\"has_read\"] == 1].copy()\n",
    "    global_mean = float(read[\"rating\"].mean()) if len(read) else float(e[\"rating\"].mean())\n",
    "\n",
    "    # user\n",
    "    u = (e.groupby(\"user_id\")\n",
    "           .agg(u_cnt=(\"book_id\",\"count\"),\n",
    "                u_read=(\"has_read\",\"sum\"),\n",
    "                u_last_ts=(\"timestamp\",\"max\"))\n",
    "           .reset_index())\n",
    "    u[\"u_plan\"] = u[\"u_cnt\"] - u[\"u_read\"]\n",
    "    u[\"u_read_ratio\"] = (u[\"u_read\"] + 1) / (u[\"u_cnt\"] + 2)\n",
    "    u[\"u_days_since_last\"] = (T - u[\"u_last_ts\"]).dt.total_seconds()/86400.0\n",
    "    u[\"u_days_since_last\"] = u[\"u_days_since_last\"].fillna(u[\"u_days_since_last\"].median())\n",
    "\n",
    "    uR = (read.groupby(\"user_id\")[\"rating\"]\n",
    "            .agg(u_read_mean=\"mean\", u_read_std=\"std\", u_read_cnt=\"count\")\n",
    "            .reset_index())\n",
    "    uHi = (read.assign(hi=(read[\"rating\"] >= 8).astype(int))\n",
    "             .groupby(\"user_id\")[\"hi\"].mean()\n",
    "             .rename(\"u_hi_frac\")\n",
    "             .reset_index())\n",
    "    u = u.merge(uR, on=\"user_id\", how=\"left\").merge(uHi, on=\"user_id\", how=\"left\")\n",
    "    for c in [\"u_read_mean\",\"u_read_std\",\"u_read_cnt\",\"u_hi_frac\"]:\n",
    "        u[c] = u[c].fillna(0)\n",
    "\n",
    "    # book\n",
    "    b = (e.groupby(\"book_id\")\n",
    "           .agg(b_cnt=(\"user_id\",\"count\"),\n",
    "                b_read=(\"has_read\",\"sum\"),\n",
    "                b_last_ts=(\"timestamp\",\"max\"))\n",
    "           .reset_index())\n",
    "    b[\"b_plan\"] = b[\"b_cnt\"] - b[\"b_read\"]\n",
    "    b[\"b_read_ratio\"] = (b[\"b_read\"] + 1) / (b[\"b_cnt\"] + 2)\n",
    "    b[\"b_pop_log\"] = np.log1p(b[\"b_cnt\"])\n",
    "    b[\"b_days_since_last\"] = (T - b[\"b_last_ts\"]).dt.total_seconds()/86400.0\n",
    "    b[\"b_days_since_last\"] = b[\"b_days_since_last\"].fillna(b[\"b_days_since_last\"].median())\n",
    "\n",
    "    bR = (read.groupby(\"book_id\")[\"rating\"]\n",
    "            .agg(b_read_mean=\"mean\", b_read_cnt=\"count\")\n",
    "            .reset_index())\n",
    "    bR[\"b_bayes_mean\"] = (bR[\"b_read_mean\"]*bR[\"b_read_cnt\"] + global_mean*BAYES_M) / (bR[\"b_read_cnt\"] + BAYES_M)\n",
    "\n",
    "    bHi = (read.assign(hi=(read[\"rating\"] >= 8).astype(int))\n",
    "             .groupby(\"book_id\")[\"hi\"].mean()\n",
    "             .rename(\"b_hi_frac\")\n",
    "             .reset_index())\n",
    "\n",
    "    b = b.merge(bR, on=\"book_id\", how=\"left\").merge(bHi, on=\"book_id\", how=\"left\")\n",
    "    for c in [\"b_read_mean\",\"b_read_cnt\",\"b_bayes_mean\",\"b_hi_frac\"]:\n",
    "        b[c] = b[c].fillna(0)\n",
    "\n",
    "    # user-book (только “видел раньше” + давность, без утечки)\n",
    "    ui = (e.groupby([\"user_id\",\"book_id\"])\n",
    "            .agg(ui_cnt=(\"timestamp\",\"count\"),\n",
    "                 ui_prev_read=(\"has_read\",\"max\"),\n",
    "                 ui_last_ts=(\"timestamp\",\"max\"))\n",
    "            .reset_index())\n",
    "    ui[\"ui_seen\"] = 1\n",
    "    ui[\"ui_days_since_last\"] = (T - ui[\"ui_last_ts\"]).dt.total_seconds()/86400.0\n",
    "    ui[\"ui_days_since_last\"] = ui[\"ui_days_since_last\"].fillna(ui[\"ui_days_since_last\"].median())\n",
    "\n",
    "    uiR = (read.groupby([\"user_id\",\"book_id\"])[\"rating\"]\n",
    "             .agg(ui_read_mean=\"mean\", ui_read_cnt=\"count\")\n",
    "             .reset_index())\n",
    "    ui = ui.merge(uiR, on=[\"user_id\",\"book_id\"], how=\"left\")\n",
    "    ui[\"ui_read_mean\"] = ui[\"ui_read_mean\"].fillna(0)\n",
    "    ui[\"ui_read_cnt\"] = ui[\"ui_read_cnt\"].fillna(0)\n",
    "\n",
    "    return u, b, ui\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Rating-aware SVD (MF-signal)\n",
    "# =========================\n",
    "def fit_svd(events: pd.DataFrame):\n",
    "    if not (USE_SVD and HAS_SVD):\n",
    "        return None\n",
    "\n",
    "    e = events[[\"user_id\",\"book_id\",\"has_read\",\"rating\"]].copy()\n",
    "    e[\"user_id\"] = e[\"user_id\"].astype(int)\n",
    "    e[\"book_id\"] = e[\"book_id\"].astype(int)\n",
    "    e[\"has_read\"] = e[\"has_read\"].astype(int)\n",
    "    e[\"rating\"] = e[\"rating\"].astype(int)\n",
    "\n",
    "    uid = e[\"user_id\"].unique()\n",
    "    bid = e[\"book_id\"].unique()\n",
    "    uid2i = {u:i for i,u in enumerate(uid)}\n",
    "    bid2i = {b:i for i,b in enumerate(bid)}\n",
    "\n",
    "    u_idx = e[\"user_id\"].map(uid2i).to_numpy()\n",
    "    b_idx = e[\"book_id\"].map(bid2i).to_numpy()\n",
    "\n",
    "    val = np.where(\n",
    "        e[\"has_read\"].to_numpy() == 1,\n",
    "        1.0 + e[\"rating\"].to_numpy() / 10.0,\n",
    "        SVD_PLAN_W\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    mat = coo_matrix((val, (u_idx, b_idx)), shape=(len(uid), len(bid))).tocsr()\n",
    "    k = int(min(SVD_DIM, max(8, min(mat.shape) - 1)))\n",
    "    svd = TruncatedSVD(n_components=k, random_state=SEED)\n",
    "    U = svd.fit_transform(mat).astype(np.float32)\n",
    "    V = svd.components_.T.astype(np.float32)\n",
    "    return uid2i, bid2i, U, V\n",
    "\n",
    "\n",
    "def add_svd_features(df_pairs: pd.DataFrame, emb):\n",
    "    df = df_pairs.copy()\n",
    "    if emb is None:\n",
    "        df[\"svd_score\"] = 0.0\n",
    "        df[\"svd_tanh\"] = 0.0\n",
    "        return df\n",
    "\n",
    "    uid2i, bid2i, U, V = emb\n",
    "    u_idx = df[\"user_id\"].map(uid2i).fillna(-1).astype(int).to_numpy()\n",
    "    b_idx = df[\"book_id\"].map(bid2i).fillna(-1).astype(int).to_numpy()\n",
    "\n",
    "    score = np.zeros(len(df), dtype=np.float32)\n",
    "    mask = (u_idx >= 0) & (b_idx >= 0)\n",
    "    if mask.any():\n",
    "        score[mask] = np.sum(U[u_idx[mask]] * V[b_idx[mask]], axis=1)\n",
    "\n",
    "    df[\"svd_score\"] = score\n",
    "    df[\"svd_tanh\"] = np.tanh(score).astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Hard negatives (top svd_score in a pool)\n",
    "# =========================\n",
    "def make_user_used(train_full: pd.DataFrame):\n",
    "    return train_full.groupby(\"user_id\")[\"book_id\"].apply(lambda s: set(map(int, s.tolist()))).to_dict()\n",
    "\n",
    "def sample_hard_negs(uid, used_set, book_pool, p, emb, need=NEG_PER_USER):\n",
    "    sampled = rng.choice(book_pool, size=NEG_POOL * NEG_OVERSAMPLE, replace=True, p=p)\n",
    "    filt, seen = [], set()\n",
    "    for b in sampled:\n",
    "        b = int(b)\n",
    "        if b in used_set or b in seen:\n",
    "            continue\n",
    "        filt.append(b)\n",
    "        seen.add(b)\n",
    "        if len(filt) >= NEG_POOL:\n",
    "            break\n",
    "    if not filt:\n",
    "        return np.empty(0, dtype=int)\n",
    "\n",
    "    tmp = pd.DataFrame({\"user_id\": int(uid), \"book_id\": filt})\n",
    "    tmp = add_svd_features(tmp, emb)\n",
    "    tmp = tmp.sort_values(\"svd_score\", ascending=False).head(need)\n",
    "    return tmp[\"book_id\"].to_numpy(dtype=int)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Build LTR dataset\n",
    "# =========================\n",
    "def build_rank_dataset(history, future, train_full):\n",
    "    # positives from future: label=2 if any read else 1\n",
    "    fut = (future.groupby([\"user_id\",\"book_id\"])\n",
    "                 .agg(fut_read=(\"has_read\",\"max\"), fut_rating=(\"rating\",\"max\"))\n",
    "                 .reset_index())\n",
    "    fut[\"label\"] = np.where(fut[\"fut_read\"] == 1, 2, 1).astype(int)\n",
    "\n",
    "    # weights: рейтинг не в label, но усиливаем read-10 сильнее read-1\n",
    "    fut[\"weight\"] = np.where(fut[\"fut_read\"] == 1, 1.0 + fut[\"fut_rating\"].astype(float)/10.0, 1.0)\n",
    "\n",
    "    pos = fut[[\"user_id\",\"book_id\",\"label\",\"weight\"]].copy()\n",
    "\n",
    "    # neg sampling pool: popular books from history\n",
    "    pop = history.groupby(\"book_id\").size()\n",
    "    book_pool = pop.index.to_numpy()\n",
    "    p = np.sqrt(pop.to_numpy(dtype=float))\n",
    "    p = p / p.sum()\n",
    "\n",
    "    used_all = make_user_used(train_full)\n",
    "\n",
    "    emb = fit_svd(history)\n",
    "\n",
    "    neg_rows = []\n",
    "    for uid in pos[\"user_id\"].unique():\n",
    "        used = used_all.get(int(uid), set())\n",
    "        nb = sample_hard_negs(uid, used, book_pool, p, emb, need=NEG_PER_USER)\n",
    "        if nb.size:\n",
    "            neg_rows.append(pd.DataFrame({\n",
    "                \"user_id\": int(uid),\n",
    "                \"book_id\": nb,\n",
    "                \"label\": 0,\n",
    "                \"weight\": 1.0\n",
    "            }))\n",
    "    neg = pd.concat(neg_rows, ignore_index=True) if neg_rows else pd.DataFrame(columns=[\"user_id\",\"book_id\",\"label\",\"weight\"])\n",
    "\n",
    "    rank_df = pd.concat([pos, neg], ignore_index=True)\n",
    "    return rank_df, emb\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Merge features + train/eval or train_full\n",
    "# =========================\n",
    "def make_feature_matrix(pairs_df, u, b, ui, emb, *, use_baseline_pos=True):\n",
    "    df = (pairs_df.merge(u.drop(columns=[\"u_last_ts\"]), on=\"user_id\", how=\"left\")\n",
    "                 .merge(b.drop(columns=[\"b_last_ts\"]), on=\"book_id\", how=\"left\")\n",
    "                 .merge(ui.drop(columns=[\"ui_last_ts\"]), on=[\"user_id\",\"book_id\"], how=\"left\"))\n",
    "\n",
    "    df[\"ui_seen\"] = df[\"ui_seen\"].fillna(0).astype(int)\n",
    "    for c in [\"ui_cnt\",\"ui_prev_read\",\"ui_days_since_last\",\"ui_read_mean\",\"ui_read_cnt\"]:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    for c in [\"u_cnt\",\"u_read\",\"u_plan\",\"u_read_ratio\",\"u_days_since_last\",\"u_read_mean\",\"u_read_std\",\"u_read_cnt\",\"u_hi_frac\",\n",
    "              \"b_cnt\",\"b_read\",\"b_plan\",\"b_read_ratio\",\"b_pop_log\",\"b_days_since_last\",\"b_read_mean\",\"b_read_cnt\",\"b_bayes_mean\",\"b_hi_frac\"]:\n",
    "        df[c] = df[c].fillna(0)\n",
    "\n",
    "    df = add_svd_features(df, emb)\n",
    "\n",
    "    if use_baseline_pos:\n",
    "        # surrogate \"baseline order\" inside training lists = rank by svd_score\n",
    "        df[\"base_pos\"] = (df.groupby(\"user_id\")[\"svd_score\"]\n",
    "                            .rank(method=\"first\", ascending=False)\n",
    "                            .astype(int) - 1)\n",
    "        df[\"base_pos_inv\"] = 1.0 / (1.0 + df[\"base_pos\"])\n",
    "        df[\"base_pos_log\"] = np.log1p(df[\"base_pos\"])\n",
    "    else:\n",
    "        df[\"base_pos\"] = 0\n",
    "        df[\"base_pos_inv\"] = 1.0\n",
    "        df[\"base_pos_log\"] = 0.0\n",
    "\n",
    "    feature_cols = [\n",
    "        \"base_pos\", \"base_pos_inv\", \"base_pos_log\",\n",
    "        \"svd_score\", \"svd_tanh\",\n",
    "        \"u_cnt\",\"u_read\",\"u_plan\",\"u_read_ratio\",\"u_days_since_last\",\n",
    "        \"u_read_mean\",\"u_read_std\",\"u_read_cnt\",\"u_hi_frac\",\n",
    "        \"b_cnt\",\"b_read\",\"b_plan\",\"b_read_ratio\",\"b_pop_log\",\"b_days_since_last\",\n",
    "        \"b_read_mean\",\"b_read_cnt\",\"b_bayes_mean\",\"b_hi_frac\",\n",
    "        \"ui_seen\",\"ui_cnt\",\"ui_prev_read\",\"ui_days_since_last\",\"ui_read_mean\",\"ui_read_cnt\",\n",
    "    ]\n",
    "    df[feature_cols] = df[feature_cols].fillna(0)\n",
    "    return df, feature_cols\n",
    "\n",
    "\n",
    "def train_and_score(rank_df, history):\n",
    "    u, b, ui = build_feature_tables(history)\n",
    "    emb = fit_svd(history)\n",
    "\n",
    "    df, feature_cols = make_feature_matrix(rank_df, u, b, ui, emb, use_baseline_pos=True)\n",
    "\n",
    "    # split by users\n",
    "    users_unique = df[\"user_id\"].unique()\n",
    "    train_u, val_u = train_test_split(users_unique, test_size=0.2, random_state=SEED)\n",
    "\n",
    "    trp = df[df[\"user_id\"].isin(train_u)].copy().sort_values(\"user_id\", kind=\"mergesort\").reset_index(drop=True)\n",
    "    vap = df[df[\"user_id\"].isin(val_u)].copy().sort_values(\"user_id\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    X_train = trp[feature_cols]\n",
    "    y_train = trp[\"label\"].to_numpy()\n",
    "    w_train = trp[\"weight\"].to_numpy()\n",
    "    g_train = trp.groupby(\"user_id\").size().to_numpy()\n",
    "\n",
    "    X_val = vap[feature_cols]\n",
    "    y_val = vap[\"label\"].to_numpy()\n",
    "    w_val = vap[\"weight\"].to_numpy()\n",
    "    g_val = vap.groupby(\"user_id\").size().to_numpy()\n",
    "\n",
    "    ranker = lgb.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        ndcg_eval_at=[20],\n",
    "        n_estimators=N_EST,\n",
    "        learning_rate=LR,\n",
    "        num_leaves=NUM_LEAVES,\n",
    "        min_data_in_leaf=50,\n",
    "        feature_fraction=0.85,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=1,\n",
    "        lambda_l2=1.0,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        label_gain=[0, 1, 8],   # read >> plan >> cold\n",
    "    )\n",
    "\n",
    "    ranker.fit(\n",
    "        X_train, y_train,\n",
    "        group=g_train,\n",
    "        sample_weight=w_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_group=[g_val],\n",
    "        eval_sample_weight=[w_val],\n",
    "        callbacks=[lgb.early_stopping(200, verbose=True)],\n",
    "    )\n",
    "\n",
    "    best_iter = ranker.best_iteration_ if ranker.best_iteration_ is not None else N_EST\n",
    "    best_score = ranker.best_score_.get(\"valid_0\", {}).get(\"ndcg@20\", None)\n",
    "\n",
    "    print(f\"\\nVAL best_iteration = {best_iter}\")\n",
    "    print(f\"VAL best ndcg@20   = {best_score}\\n\")\n",
    "\n",
    "    return ranker, best_iter, feature_cols\n",
    "\n",
    "\n",
    "def train_full(rank_df, history, best_iter, feature_cols):\n",
    "    u, b, ui = build_feature_tables(history)\n",
    "    emb = fit_svd(history)\n",
    "\n",
    "    df, _ = make_feature_matrix(rank_df, u, b, ui, emb, use_baseline_pos=True)\n",
    "    df = df.sort_values(\"user_id\", kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "    X = df[feature_cols]\n",
    "    y = df[\"label\"].to_numpy()\n",
    "    w = df[\"weight\"].to_numpy()\n",
    "    g = df.groupby(\"user_id\").size().to_numpy()\n",
    "\n",
    "    ranker = lgb.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        ndcg_eval_at=[20],\n",
    "        n_estimators=int(best_iter),\n",
    "        learning_rate=LR,\n",
    "        num_leaves=NUM_LEAVES,\n",
    "        min_data_in_leaf=50,\n",
    "        feature_fraction=0.85,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=1,\n",
    "        lambda_l2=1.0,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1,\n",
    "        label_gain=[0, 1, 8],\n",
    "        verbose=500\n",
    "    )\n",
    "\n",
    "    ranker.fit(X, y, group=g, sample_weight=w)\n",
    "    return ranker\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Predict candidates\n",
    "# =========================\n",
    "def predict_candidates(model, feature_cols, train_full, candidates):\n",
    "    # features from FULL train (это то, что реально доступно на тесте)\n",
    "    uF, bF, uiF = build_feature_tables(train_full)\n",
    "    embF = fit_svd(train_full)\n",
    "\n",
    "    cand = explode_candidates(candidates)\n",
    "\n",
    "    cand = (cand.merge(uF.drop(columns=[\"u_last_ts\"]), on=\"user_id\", how=\"left\")\n",
    "               .merge(bF.drop(columns=[\"b_last_ts\"]), on=\"book_id\", how=\"left\")\n",
    "               .merge(uiF.drop(columns=[\"ui_last_ts\"]), on=[\"user_id\",\"book_id\"], how=\"left\"))\n",
    "\n",
    "    cand[\"ui_seen\"] = cand[\"ui_seen\"].fillna(0).astype(int)\n",
    "    for c in [\"ui_cnt\",\"ui_prev_read\",\"ui_days_since_last\",\"ui_read_mean\",\"ui_read_cnt\"]:\n",
    "        cand[c] = cand[c].fillna(0)\n",
    "\n",
    "    for c in [\"u_cnt\",\"u_read\",\"u_plan\",\"u_read_ratio\",\"u_days_since_last\",\"u_read_mean\",\"u_read_std\",\"u_read_cnt\",\"u_hi_frac\",\n",
    "              \"b_cnt\",\"b_read\",\"b_plan\",\"b_read_ratio\",\"b_pop_log\",\"b_days_since_last\",\"b_read_mean\",\"b_read_cnt\",\"b_bayes_mean\",\"b_hi_frac\"]:\n",
    "        cand[c] = cand[c].fillna(0)\n",
    "\n",
    "    cand = add_svd_features(cand, embF)\n",
    "\n",
    "    # IMPORTANT: в тесте baseline-позиция есть! Используем её как base_pos\n",
    "    cand[\"base_pos\"] = cand[\"orig_pos\"]\n",
    "    cand[\"base_pos_inv\"] = cand[\"orig_pos_inv\"]\n",
    "    cand[\"base_pos_log\"] = cand[\"orig_pos_log\"]\n",
    "\n",
    "    cand[feature_cols] = cand[feature_cols].fillna(0)\n",
    "    cand[\"score\"] = model.predict(cand[feature_cols])\n",
    "\n",
    "    sub = (cand.sort_values([\"user_id\",\"score\"], ascending=[True, False])\n",
    "              .groupby(\"user_id\")[\"book_id\"]\n",
    "              .apply(lambda s: \",\".join(map(str, s.to_list())))\n",
    "              .reset_index()\n",
    "              .rename(columns={\"book_id\":\"book_id_list\"}))\n",
    "    return sub\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_df, candidates = load_public(\"public/\")\n",
    "\n",
    "    history, future = split_history_future(train_df)\n",
    "    rank_df, _ = build_rank_dataset(history, future, train_df)\n",
    "\n",
    "    # 1) train+val => score\n",
    "    _, best_iter, feature_cols = train_and_score(rank_df, history)\n",
    "\n",
    "    # 2) fit on full rank_df with best_iter\n",
    "    final_model = train_full(rank_df, history, best_iter, feature_cols)\n",
    "\n",
    "    # 3) predict\n",
    "    sub = predict_candidates(final_model, feature_cols, train_df, candidates)\n",
    "    sub.to_csv(\"submission_validated.csv\", index=False)\n",
    "    print(\"Saved submission.csv:\", sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc39f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DANO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
